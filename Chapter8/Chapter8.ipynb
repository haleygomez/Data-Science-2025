{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting models to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure you have watched the Chapter 8 video(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will learn the following things in this Chapter\n",
    "\n",
    "- Analytic expressions of fitting a straight line to data: linear regression.\n",
    "- Understand how errors need to be accounted for.\n",
    "- How to use Python functions to fit curves or non linear data.\n",
    "- How to evaluate the goodness of your fit.\n",
    "- Be able to use an MCMC to fit data with a model.\n",
    "- Understand how bootstrapping can give us sampling statistics.\n",
    "- How to use Python programming to do the above.\n",
    "- After completing this notebook you will be able to finish CA2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining our data with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will review one of the most important aspects of day-to-day data science - given data and a model, what the values for the parameters in that model. In other words, **parameter estimation**, one of the most important goals of data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting straight line models to data - linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably one of the most common things scientists do, is fit a straight line to data. To do this we want to express the fit as a linear relation in the form of $y = A + Bx$ where $A$ is the y-intercept and $B$ is the gradient ie the true value of $y_i$ is given by $y_i = A+Bx_i$. However we have to be careful with regards to errors in $y$ or $x$.\n",
    "\n",
    "Normally what happens is that the experimenters make a scatter plot of $x$ and $y$, and notice that there's some sort of linear-looking relationship, and decide it would be good to have a mathematical form for this relationship, and so try to fit a straight line. They may even have calculated the covariance or correlation statistics from a data set, and realised that two of the variables have a strong correlation. How do we determine the best fit line to find the values of A and B parameters that describe our data and give us the “true y”?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation: y=A+Bx parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we are going to look towards the principle of maximum likelihood for help. If we knew what $A$ and $B$ were, we could, for a given value of $x_i$, compute the corresponding value of $y_i$,\n",
    "\n",
    "$\\text{true value for}y_i=A+Bx_i$.\n",
    "\n",
    "The measured values of $y_i$ are therefore drawn from a normal distribution of width $\\sigma_y$ that is centred on the true value $y_i$. So we can write the probability of obtaining any given measurement $y_i$ as,\n",
    "\n",
    "$p_{A,B}(y_i)\\propto \\dfrac{1}{\\sigma_y}e^{-(y_i-A-Bx_i)^2/2\\sigma_y^2}.$\n",
    "\n",
    "The subscripts $A$ and $B$ indicate that this probability depends on the unknown values of $A$ and $B$.\n",
    "\n",
    "Taking this a stage further, we can now write the probability of obtaining our entire set of measurements as\n",
    "\n",
    "$p_{A,B}(y_1,y_2,\\ldots,y_N)=p_{A, B}(y_1)\\dotsb p_{A,B}(y_N)$\n",
    "$p_{A,B}(y_1,y_2,\\ldots,y_N)\\propto \\frac{1}{\\sigma_y^N}e^{-\\chi^2/2}$\n",
    "\n",
    "where $\\chi^2$ is our old friend from Chapter 5,\n",
    "\n",
    "$\\chi^2=\\sum_{i=1}^{N}\\dfrac{(y_i-A-Bx_i)^2}{\\sigma_y^2}.$\n",
    "\n",
    "As before, we are interested in recovering the values of $A$ and $B$ that maximise the probability of obtaining the set of the observed measurements, $(x_i,y_i)$. The joint probability of the measurements is obtained when $\\chi^2$ is a minimum, so we need to differentiate with respect to our unknowns $A$ and $B$, and set the differentials equal to zero, as we did before:\n",
    "\n",
    "$\\dfrac{\\partial \\chi^2}{\\partial A}=\\dfrac{-2}{\\sigma_y^2}\\sum_{i=1}^N (y_i-A-Bx_i)=0$\n",
    "\n",
    "\n",
    "$\\dfrac{\\partial \\chi^2}{\\partial B}=\\dfrac{-2}{\\sigma_y^2}\\sum_{i=1}^N x_i(y_i-A- Bx_i)=0.$\n",
    "\n",
    "This results in a pair of simultaneous equations for  $A$ and $B$,\n",
    "\n",
    "$AN+B\\sum x_i=\\sum y_i$,\n",
    "\n",
    "and\n",
    "\n",
    "$A\\sum x_i+B\\sum x_i^2=\\sum x_i y_i$,\n",
    "\n",
    "where we have dropped the limits on the sumation for sake of clarity.\n",
    "\n",
    "These equations can then be solved for $A$ and $B$ to get,\n",
    "\n",
    "$A=\\dfrac{\\sum x^2\\sum y-\\sum x\\sum xy}{N\\sum x^2-(\\sum x)^2}$\n",
    "\n",
    "\n",
    "$B=\\dfrac{N\\sum xy-\\sum x\\sum y}{N\\sum x^2-(\\sum x)^2}.$\n",
    "\n",
    "Together, these equations allow us to calculate the best fit to the line $y=A+Bx$. The resulting line called the least squares fit, or line of regression of $y$ on $x$. Now that we have $A$ and $B$, we naturally want to estimate the uncertainties in these values. But first we need to discuss the uncertainty in $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors in y - constant error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis above, we assumed that the values of $y_i$ were distributed around the true values with spread of $\\sigma_y$. You will notice however, that our estimates of $A$ and $B$ do not actually depend on this number (or numbers, in the case where $\\sigma_y$ is different for each point). However the true values of $y_i$ are predicted  by the line which $A$ and $B$ describe, and so the deviations of our measured values $y_i$ should be depend on $A$ and $B$. This immediately suggests that a good way to estimate $\\sigma_y$ is from\n",
    "\n",
    "$\\sigma_y =\\sqrt{\\dfrac{1}{N}\\sum_{i = 1}^N(y_i-A-Bx_i)^2}$\n",
    "\n",
    "\n",
    "which is the usual root-mean-square deviation from the mean, where in this case, the mean is defined by the best estimate of the line we have just fitted (i.e. the values $A$ and $B$).  However, the equations above have a problem, it violates the degrees of freedom condition. Since both $A$ and $B$ have been defined from the data already, we need to replace the $\\dfrac{1}{N}$ with $\\dfrac{1}{N -2}$ to get,\n",
    "\n",
    "$\\sigma_y=\\sqrt{\\dfrac{1}{N-2}\\sum_{i=1}^N(y_i-A-Bx_i)^2}$.\n",
    "\n",
    "This makes sense: if you have only two data points, then the best guess for the line will have $A$ and $B$ such that the line goes exactly through both points - there is no scatter! As soon as we have 3 data points, then at least 1 will not sit on the line, and the idea of $\\sigma_y$ becomes meaningful.\n",
    "\n",
    "How does this propogate into the uncertainty in $A$ and $B$? Note that both $A$ and $B$ are well-defined functions of $y_i$. This means that we can use the standard error propagation formula that we encountered Chapter 5. Using this, we get,\n",
    "\n",
    "$\\sigma_A=\\sigma_y\\sqrt{\\dfrac{\\sum x^2}{N\\sum x^2-(\\sum x)^2}}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\sigma_B=\\sigma_y\\sqrt{\\dfrac{N}{N\\sum x^2-(\\sum x)^2}}$.\n",
    "\n",
    "These kind of errors are known as homoscedastic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors in both x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if there are uncertainties on both $x$ and $y$? For the special case of straight line, the uncertainties in both $x$ and $y$ make very little difference. To see this imagine that there is no error in $y$, but $x$ has an error of $\\delta x$. This point lies off the line, at a distance $\\delta y$ so that ultimately it produces an equivalent error in $y$. For the case of a straight line, the relationship is,\n",
    "\n",
    "$\\delta y(\\text{equiv})=\\dfrac{dy}{dx}\\delta x.$\n",
    "\n",
    "The standard deviation $\\sigma_x$ is the root-mean-square value of $ \\delta x$ that would result from repeating this measurement for each of our points, and so,\n",
    "\n",
    "$\\sigma_y(\\text{equiv})=\\dfrac{dy}{dx}\\sigma_x.$\n",
    "\n",
    "This is true for any function that has no second derivative, but in the special case of a straight line, $\\dfrac{dy}{dx}$ is simply our fit parameter, $B$. Thus the problem of fitting a line with uncertainties in $x$ but none in $y$ is the same as the problem of fitting a line with uncertainties in $y$ but none in $x$.\n",
    "\n",
    "In the case where there are uncertainties in both $x$ and $y$, we simply translate the $\\sigma_x$ to $\\sigma_y$. Accounting for the intrinsic errors in $y$, this yields,\n",
    "\n",
    "$\\sigma_y(\\text{equiv})=\\sqrt{\\sigma_y^2+(B \\sigma_x)^2}$\n",
    "\n",
    "where we have combined the errors in quadrature in the usual way. Clearly, if we are not interested in fitting a straight line, then this expression may be more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heteroscedastic errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the case in which the errors on $y_i$ are not constant? (These are called heteroscedastic errors.) In this case we need to use the idea of **weighted least squares** (see Chapter 5). By carrying the weights $w_i = 1/\\sigma_i^2$ (these are on $y_i$) through the analysis, one ends up with the following expressions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight the probabilities by the errors where $w = 1/\\sigma^2$\n",
    "\n",
    "$A = \\dfrac{\\sum wx^2 \\sum wy - \\sum wx \\sum wxy }{\\sum w \\sum wx^2 - (\\sum wx)^2}$\n",
    "\n",
    "$B = \\dfrac{\\sum w \\sum wxy - \\sum wx \\sum wy }{\\sum w \\sum wx^2 - (\\sum wx)^2}$.\n",
    "\n",
    "The associated uncertainties are,\n",
    "\n",
    "$\\sigma_A =  \\sqrt{ \\dfrac{\\sum wx^2}{  \\sum w \\sum wx^2 - (\\sum wx)^2} }$\n",
    "\n",
    "and\n",
    "\n",
    "$\\sigma_B =  \\sqrt{ \\dfrac{\\sum w} {  \\sum w \\sum wx^2 - (\\sum wx)^2} }$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically see that every term in the homoscedastic errors has an extra $w$ and $N$ is replaced by $\\sum w$.  Just like a weighted average, points contribute to the end sum where less weight is given to the\n",
    "less precise measurements and more weight given to more precise measurements. This approach assumes that $w$ is the exact weight which it is not (worse if derived from small samples) and this approach can still be sensitive to outliers.   The concept of weighting can be used in both linear and non linear fitting, though the expressions above are only for the linear case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "Netflix managers poll their subscribers to see how the fraction of users that watched She-Ra in October 2019 $x$ compares with the age of the viewer $y$ before they decide to commission a new series of the show. This data is available [here](https://github.com/haleygomez/Data-Science-2025/raw/main/blended_exercises/Chapter8/DataScience_datafile_1.dat), url: https://github.com/haleygomez/Data-Science-2025/raw/main/blended_exercises/Chapter8/DataScience_datafile_1.dat. \n",
    "\n",
    "1. Write functions from scratch to fit a straight line to the data quoting all the fit parameters and their errors.  \n",
    "\n",
    "2. Comment on the uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This questions involves fitting a straight line to set of points with homoscedastic errors on $y$. \n",
    "\n",
    "Since the errors are homoscedastic, the scatter around the line should be notably larger than the $\\sigma = 1.6$ that one might expect. \n",
    "\n",
    "Let's open the datafile and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-12 15:22:46--  https://github.com/haleygomez/Data-Science-2025/raw/main/blended_exercises/Chapter8/DataScience_datafile_1.dat\n",
      "Resolving github.com (github.com)... 20.26.156.215\n",
      "Connecting to github.com (github.com)|20.26.156.215|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-09-12 15:22:47 ERROR 404: Not Found.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/027r1smj7d3bpgk4j6fcpcgw0000gs/T/ipykernel_22793/1825945536.py:4: UserWarning: genfromtxt: Empty input file: \"data_mcmc.dat\"\n",
      "  data = np.genfromtxt('data_mcmc.dat',names=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yb/027r1smj7d3bpgk4j6fcpcgw0000gs/T/ipykernel_22793/1825945536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_mcmc.dat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print out the columns to see what's in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;31m# Should we take the first values as names ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m             \u001b[0mfval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget -O data_mcmc.dat \"https://github.com/haleygomez/Data-Science-2025/raw/main/blended_exercises/Chapter8/DataScience_datafile_1.dat\"\n",
    "\n",
    "import numpy as np\n",
    "data = np.genfromtxt('data_mcmc.dat',names=True)\n",
    "\n",
    "# print out the columns to see what's in the file\n",
    "print(data.dtype.names)\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt \n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "#data = np.genfromtxt('data_chapter8.dat',names=True)\n",
    "\n",
    "plt.errorbar(data['x'],data['y'],yerr=data['sigma_y_estimate'],fmt='o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter in the data is clearly much larger than the errors in $y$ quoted!  Note though, to orthogonalise the model we need to either use the above equations and **quote the covariance** or use:\n",
    "\n",
    "$y = A+B(x_i-\\hat{x})$\n",
    "\n",
    "throughout, where $\\hat{x}$ is the mean.  Here we will show the 1st option.\n",
    "\n",
    "The errors are homoscedastic since the errors on y are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to estimate A and B based on notes and equations above\n",
    "def best_fit_a(x,y,N):\n",
    "    result = (np.sum(x**2.)*np.sum(y) - np.sum(x)*np.sum(x*y))/ ((N*np.sum(x**2.)) - (np.sum(x))**2. )\n",
    "    return result\n",
    "\n",
    "def best_fit_b(x,y,N):\n",
    "    result = ((N*np.sum(x*y))- (np.sum(x)*np.sum(y))) / ((N*np.sum(x**2.)) - (np.sum(x))**2. )\n",
    "    return result\n",
    "\n",
    "A = best_fit_a(data['x'],data['y'],len(data['x']))\n",
    "B = best_fit_b(data['x'],data['y'],len(data['x']))\n",
    "\n",
    "print('Best fit line equation paramaters: A = {:.2f} age in years'.format(A),'and B= {:.2f} age in years/fraction'.format(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot the straightline\n",
    "def straightline(a,b,x):\n",
    "    result = a + b*(x)\n",
    "    return result\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "fit = straightline(A,B,x)\n",
    "\n",
    "plt.errorbar(data['x'],data['y'],yerr=data['sigma_y_estimate'],fmt='o')\n",
    "plt.plot(x,fit,lw=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For errors in $A$ and $B$ we need to use the standard deviation of the $y$ data:\n",
    "\n",
    "$\\sigma_y = \\sqrt{ \\dfrac{1}{N} \\sum^N_{i-1} (y_i - A-Bx_i)^2 }$\n",
    "\n",
    "where here $N$ is the *number of degrees of freedom* ie in this case $N={\\rm number~of~data~ points}-2$. The $N-2$ appears because we already need to know $A$ and $B$ to work out $\\sigma_y$.\n",
    "\n",
    "We need to do this because the errors on the fit depends not just on the error bars in $y$ but also on the fit itself, and by using the above equation we can estimate the true error in $y$ (which should be larger than then 1.6 errors given to the students for each datapoint).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's estimate sigma_y using our A+B values\n",
    "\n",
    "def sigma(N,A,B,x,y):\n",
    "    sigma = np.sqrt( (1./(N-2))* np.sum((y - A - B*x)**2.0)  )\n",
    "    return sigma\n",
    "\n",
    "N=len(data['x'])\n",
    "x= data['x']\n",
    "y= data['y']\n",
    "\n",
    "sigma_y = sigma(N,A,B,x,y)\n",
    "\n",
    "print('The error in y (sigma_y) from our best fit line is {:.2f}.'.format(sigma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the $\\sigma_y$ of the data is much larger than the 1.6 on each data point quoted in the original data file. \n",
    "\n",
    "Calculating $\\sigma_A$ and $\\sigma_B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_in_a(x,sig_y,N):\n",
    "    result = sig_y* np.sqrt( np.sum(x**2.) / ((N*np.sum(x**2.)) - (np.sum(x))**2.  )  )\n",
    "    return result\n",
    "\n",
    "def error_in_b(x,sig_y,N):\n",
    "    result = sig_y* np.sqrt( N / ((N*np.sum(x**2.)) - (np.sum(x))**2.  )  )\n",
    "    return result\n",
    "\n",
    "error_A = error_in_a(data['x'],sigma_y,len(data['x']))\n",
    "error_B = error_in_b(data['x'],sigma_y,len(data['x']))\n",
    "\n",
    "print('A is {:.2f} age in years'.format(A),'with error {:.2f}'.format(error_A))\n",
    "print('B is {:.2f} age in years/per fraction'.format(B),'with error {:.2f}'.format(error_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is not a good fit if we take the error in $y$ from the original data, but the fit vs model is better when accounting for the larger error in $y$ compared to the true fit $\\sigma_y$. But this is simply because the error is so large the model can be seen as an ok fit\n",
    "\n",
    "One can also try the Spearman rank correlation to see if a straight line fit is appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "# let's rank the data\n",
    "r_x = rankdata(data['x'])\n",
    "r_y = rankdata(data['y'])\n",
    "\n",
    "# need to set up equation in notes\n",
    "def rho_s(rank_x,rank_y,N):\n",
    "    top = np.sum(rank_x*rank_y) - (N*(N+1)**2./4)\n",
    "    bottom_1 = np.sqrt( np.sum(rank_x**2.) - (N*(N+1)**2./4)   )  \n",
    "    bottom_2 = np.sqrt( np.sum(rank_y**2.) - (N*(N+1)**2./4)   )\n",
    "    rho = top / (bottom_1*bottom_2)\n",
    "    return rho\n",
    "\n",
    "spearman = rho_s(r_x,r_y,len(data['x']))\n",
    "\n",
    "print('the Spearman rank statistic is {:.2f}'.format(spearman))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus a linear relationship between the two does not make a good model for this data since the Spearman rank statistic is less than 0.6.   As the model chosen to fit the data is flawed the error on the fit parameters found from the equations above are in fact too small and does not accurately reflect the uncertainty in the $A$ and $B$ values given the quality of the fit. \n",
    "\n",
    "We did all of the above by hand to make sure we could work through the mathematics, but we can also simply use Python to make things easier and quicker.  For example, we could fit a straight line using the functions `poly1d` or `curve_fit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize \n",
    "\n",
    "# this is a function that we think may explain the data\n",
    "def straightline_func(x,a,b):\n",
    "    return a + b*x\n",
    "\n",
    "data['x'],data['y']\n",
    "\n",
    "params, params_covariance = optimize.curve_fit(straightline_func, data['x'],data['y'],\\\n",
    "                                               sigma=data['sigma_y_estimate'],absolute_sigma=True)\n",
    "\n",
    "print('Fit parameters A and B are {:.3f} and {:.3f}'.format(params[0],params[1]))\n",
    "print()\n",
    "print('covariance matrix',params_covariance)\n",
    "\n",
    "# plot the data\n",
    "plt.errorbar(data['x'],data['y'],yerr=data['sigma_y_estimate'],fmt='o',label='data')\n",
    "plt.xlabel('time (days')\n",
    "plt.ylabel('brightness of star')\n",
    "\n",
    "# plot the fit\n",
    "plt.plot(data['x'],straightline_func(data['x'],params[0],params[1]),lw=2,label='best fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by hand\n",
    "print('Best fit line equation paramaters by hand: A = {:.2f} age in years'.format(A),\\\n",
    "          'and B= {:.2f} age in years/fraction'.format(B))\n",
    "\n",
    "# using Python's curve fit\n",
    "\n",
    "print('Best fit line equation paramaters using functions: A = {:.2f} age in years'.format(params[0]),'and B= {:.2f} age in years/fraction'.format(params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Good a Fit is Our Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the Netflix data above and take a look at how *different* our model predictions are (in this case our straight line with $A$ and $B$) compared with the original data  - this difference is called the *residual*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual = difference between data and model\n",
    "residual = data['y'] - straightline(A,B,data['x'])\n",
    "\n",
    "plt.scatter(data['x'],residual)\n",
    "\n",
    "plt.ylim(-10,10)\n",
    "plt.ylabel('residual age')\n",
    "plt.xlabel('fraction of Netflix users')\n",
    "plt.axhline(0,c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see just by looking at the residuals between the data and what would be predicted by our model, that there is a lot of scatter above and below the zero line (which is what we would get if our model matched our data perfectly). The scatter does not appear randomly distributed either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced chi-square test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots we've made above, the straight line does not look like a good fit to the huge scatter around the data - indeed the straight line does not even pass though many of the error bars on the data points.  This suggests that either there are unaccounted sources of error or that the data is poorly modelled with a straight-line.  We can quantify whether our fit is good by using the information provided by the residuals and estimating the chi-square statistic (Chapter 5).\n",
    "\n",
    "The reduced chi-square test to evaluate the goodness of fit is given by\n",
    "\n",
    "$\\bar{\\chi}^2 = \\dfrac{ \\chi^2 }{ N_d}$\n",
    "\n",
    "where $N_d$ is the number of degrees of freedom = $N_{\\rm data} - N_{\\rm params}$. \n",
    "\n",
    "Looking at the plots we've made above, the straight line does not look like a good fit to the huge scatter around the data - indeed the straight line does not even pass though many of the error bars on the data points.  This suggests that either there are unaccounted sources of error or that the data is poorly modelled with a straight-line. \n",
    "\n",
    "If the model describes the data and we have a good estimate of the uncertainty of each data point, then the reduced chi-square value would be approximately equal to one. \n",
    "\n",
    "There might be fluctuation around one due to random variations about the mean value, so we should use an additional criteria to decide whether to reject the null hyopthesis (that the model describes the data and any deviations are explained due to random variation).  Given that the mean of a chi-square distribution is $\\chi_{\\rm mean}^2 = N_d$ and the variance is $2 N_d$:\n",
    "\n",
    "$N_d \\pm 2\\sqrt{2N_d} \\le \\chi^2_{\\rm mean} \\le N_d \\pm 2\\sqrt{2N_d}$,\n",
    "\n",
    "ie is your value for $\\chi^2$ within $\\pm 2\\sigma$ of the mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence intervals on the chi-square statistic\n",
    "\n",
    "We can also estimate confidence limits for chi-square parameters. The  region of confidence (significance level $\\alpha$) is defined by\n",
    "\n",
    "$\\chi_{\\alpha}^2 = \\chi_{\\rm min}^2 + \\Delta$\n",
    "\n",
    "The following table shows the value of $\\Delta$ (difference above value of $\\chi^2$) for N fit parameters for a given confidence interval (1, 2 and 3 $\\sigma$).\n",
    "\n",
    "| probability  |  N=1   | N=2 | N=3 |\n",
    "|--------------|--------| ----| ----| \n",
    "| 0.680 | 1.00| 2.30 | 3.53 |\n",
    "|0.954 | 4.00| 6.17 | 8.02|\n",
    "|0.997 | 9.00 | 11.8 | 14.20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "In the question above, we fit the Netflix data with a straight line.  Derive a measure of the goodness of fit of this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_{\\rm data} - N_{\\rm params}$ is $100-2$ since we need $A$ and $B$ (which are not independent) and $\\sigma_y$ to determine $\\chi^2$. We would expect reduced $\\chi^2$ to equal close to 1 to denote a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn(a,b,z):\n",
    "    return a + b*z\n",
    "\n",
    "def chi_sq(y,x,sig_y,a,b):\n",
    "    result = np.sum(  (y - a-b*data['x'])**2. /sig_y**2. )\n",
    "    return result\n",
    "\n",
    "sig_y = 1.6 # quoted as error in the data\n",
    "N_d = len(data['x']) - 2 # degrees of freedom: need to calculate A+B + sigma\n",
    "chi_sq = chi_sq(data['y'],data['x'],sig_y,A,B)\n",
    "\n",
    "red_chi_sq  = chi_sq / N_d \n",
    "err_red_chi_sq = np.sqrt(2./len(data['x']))\n",
    "\n",
    "print('chi-square is {:.3f} +/- {:.3f}'.\\\n",
    "      format(chi_sq,np.sqrt(2*N_d)))\n",
    "\n",
    "print()\n",
    "print('reduced chi-square is {:.3f} +/- {:.3f}'.\\\n",
    "      format(red_chi_sq,np.sqrt(2/err_red_chi_sq)))\n",
    "\n",
    "# check if it's within 2sigma of the mean value of chi\n",
    "chi_sq_mean = N_d\n",
    "print()\n",
    "print('chi-square mean is {:.3f}'.\\\n",
    "      format(chi_sq_mean))\n",
    "\n",
    "crit = 2*np.sqrt(2*N_d)\n",
    "\n",
    "print()\n",
    "print('chi-square mean +/- 2sigma is {:.3f} +{:.3f} -{:.3f}'.\\\n",
    "      format((chi_sq_mean),(chi_sq_mean+crit),(chi_sq_mean-crit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore this qualitively proves that our straight line model is not a good fit, as the chi-square statistic we calculated is *not* within 2$\\sigma$ of the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have good reason to think that nature generates values of some physical quantity  $y$  in a way that depends on some other physical quantity  $x$, in the manner of some model, let's say: $y(x)=Ax+B$.  We would like to find out the values $y$ and ranges of $A$ and $B$ by taking a number of measurements of $y(x)$ to trace the relationship. Suppose also that there is some uncertainty in each of the measurements. \n",
    "\n",
    "To explore how our measurements might behave, we can simulate a large number of hypothetical experiments by using Monte Carlo methods and assess the range of parameters returned from these simulated or \"fake\" experiments.\n",
    "\n",
    "We can approach this by simulating how much each measured $y_i$ we've generated, deviates from the \"true\" value $g_i$ which our model (in the example above, the straight line) predicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "Use the Monte Carlo method to generate random data from the Netflix data in DataScience_datafile1.dat, and derive new (straight line) fit parameters $A$ and $B$ from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# generate n sets of data from the datafile by adding gaussian noise within error\n",
    "n_sample = 1000 \n",
    "n = len(data['x'])\n",
    "\n",
    "new_y =np.zeros([n,n_sample])\n",
    "new_x =np.zeros([n,n_sample])\n",
    "\n",
    "for i in range(n_sample):\n",
    "    new_y[:,i] = np.random.normal(data['y'],data['sigma_y_estimate']) \n",
    "    new_x[:,i] =  data['x']\n",
    "    \n",
    "A_mc = np.zeros(n_sample)\n",
    "B_mc = np.zeros(n_sample)\n",
    "\n",
    "# fit a line to each set of data\n",
    "for j in range(n_sample):\n",
    "    A_mc[j] = best_fit_a(new_x[:,j],new_y[:,j],n)\n",
    "    B_mc[j] = best_fit_b(new_x[:,j],new_y[:,j],n)  \n",
    "\n",
    "# get average values of A and B from all the samples\n",
    "mean_A_mc = np.mean(A_mc)\n",
    "mean_B_mc = np.mean(B_mc)\n",
    "\n",
    "print('Best fit line from earlier: A = {:.2f} age in years'.format(A),\\\n",
    "          'and B= {:.2f} age in years/fraction'.format(B))\n",
    "print('Best fit line from MC: A = {:.2f} age in years'.format(mean_A_mc),\\\n",
    "          'and B= {:.2f} age in years/fraction'.format(mean_B_mc))\n",
    "\n",
    "plt.scatter(A_mc,B_mc,marker='o',c='blue',alpha=0.5)\n",
    "plt.scatter(A,B,c='pink',label = 'Data results',s=100)\n",
    "plt.scatter(mean_A_mc,mean_B_mc,c='magenta',label = 'MC results',s=100)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit values from the straight line fit to the original data and the straight line fit to the MC data are rather similar.   But the great thing about the MC is that we can use the standard deviation to derive an uncertainty in $A$ and $B$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_A_mc_err = np.std(A_mc,ddof=1)\n",
    "mean_B_mc_err = np.std(A_mc,ddof=1)\n",
    "\n",
    "print('Best fit line equation paramaters from original data: A = {:.2f} +/- {:.2f} age in years'.format(A,error_A),\\\n",
    "          'and B= {:.2f} +/- {:.2f} age in years/fraction'.format(B,error_B))\n",
    "print()\n",
    "print()\n",
    "print('Best fit line equation paramaters from MC: A = {:.2f} +/- {:.2f} age in years'.format(mean_A_mc,mean_A_mc_err),\\\n",
    "          'and B= {:.2f} +/- {:.2f} age in years/fraction'.format(mean_B_mc,mean_B_mc_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot all our best fit lines to take a look at them\n",
    "for i in range(n_sample):\n",
    "    fit_new = straightline(A_mc[i],B_mc[i],x)\n",
    "    plt.plot(x,fit_new,c='grey',alpha=0.2)\n",
    "    \n",
    "# straight line fit from mean A and B from MC analysis\n",
    "fit_mc = straightline(mean_A_mc,mean_B_mc,x)\n",
    "fit_data = straightline(A,B,x)\n",
    "# straight line fit from original data\n",
    "\n",
    "plt.plot(x,fit_mc,lw=6,c='magenta',label='best fit to MC')\n",
    "plt.plot(x,fit_data,lw=2,label='best fit to original data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting curves to data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to fit curves is via `the scipy` `optimize.curve_fit`. This returns an array with the best fitting parameters for the function it is given, based on minimising the differences in the `func(x,fit parameters)-y` squared.\n",
    "\n",
    "It also returns an array with the covariance of the fit parameters (see Chapter 4). The diagonals of this array also provides the variance of the parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "1. Generate some fake data and fit a curve to it using the `scipy` `optimize curvefit` function and an equation of the form $y = A {\\rm sin}(bx)+C$. Perturb each $y$ value by adding on some $\\Delta y$ to create new \"noisy\" data.   Obtain errors in $y$ using a random normal distribution with normalisation of 20% of $y$.\n",
    "\n",
    "2. Derive the reduced chi-square value for your fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import pylab as plt \n",
    "%matplotlib inline  \n",
    "\n",
    "# this is a function that we think may explain the data\n",
    "def test_func(x, a,b,c):\n",
    "    return a * np.sin(b * x)+c\n",
    "\n",
    "# generate some fake data\n",
    "# start off with x array\n",
    "xdata = np.linspace(0, 4, 50)\n",
    "# get y from the function above\n",
    "y = test_func(xdata, 2.5, 1.3, 0.5)\n",
    "\n",
    "# initialise the random_seed for reproducability\n",
    "# ie we will get the same initial random seed every time\n",
    "np.random.seed(1729)\n",
    "# perturb y by this noise\n",
    "y_noise = 0.2*np.random.normal(size=xdata.size)\n",
    "# make new y array that is noisy\n",
    "ydata = y + y_noise\n",
    "\n",
    "# set up y error \n",
    "y_sigma = 0.2*(0.5+np.random.normal(size=xdata.size))\n",
    "\n",
    "# plot the data\n",
    "plt.errorbar(xdata, ydata,yerr=y_sigma,fmt='o',label='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scipy to do the fitting for us\n",
    "# need to include y errors\n",
    "params, params_covariance = optimize.curve_fit(test_func, xdata, ydata,sigma=y_sigma,absolute_sigma=True)\n",
    "print('Fit parameters A, B and C are {:.3f}, {:.3f} and {:.3f}'.format(params[0],params[1],params[2]))\n",
    "print()\n",
    "print('covariance matrix',params_covariance)\n",
    "\n",
    "# plot the data\n",
    "plt.errorbar(xdata, ydata,yerr=y_noise,fmt='o',label='data')\n",
    "plt.xlabel('time (days')\n",
    "plt.ylabel('brightness of star')\n",
    "\n",
    "# plot the fit\n",
    "plt.plot(xdata,test_func(xdata,params[0],params[1],params[2]),lw=2,label='best fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "# equation to get reduced chi^2\n",
    "def chi_sq_red(x,y,y_error):\n",
    "    # number of degrees of freedom\n",
    "    n = len(x) - 3 #3 parameters already need to be calculated : a, b + c\n",
    "    result = (1./n)*np.sum(((y-test_func(x,params[0],params[1],params[2]))/y_error)**2.0) \n",
    "    return result\n",
    "\n",
    "# calculate chi^2 of the fit:\n",
    "print('The reduced chi-squared for the fit is {:.3f}'.format(chi_sq_red(xdata,ydata,y_sigma))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches for non linear relationships or non-normal errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propogation of errors in complex systems can quickly get complicated. Sometimes the error terms are non-linear in the fit parameters, and cannot be analytically solved to give the best fitting model. To get around this we can use two powerful computatonal techniques to deal with this problem, and that can return reliable results - the MCMC and the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using MCMC to derive the best fitting model parameters between model and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we **make chi-square the distribution of space we need to sample**.\n",
    "\n",
    "Here your likelihood is the chi-square function - best to use so-called “log likelihood” given the exponential term can get tricky with computers. The log likelihood for chi-square is given by\n",
    "\n",
    "> log likelihood $=-0.5\\dfrac{\\sum{(y-y_{\\rm model})}^2}{\\sigma^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation: the likelihood for a chi-square "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a chi-square distribution with a Gaussian error distribution, the probability of each point for chi-square is written as:\n",
    "\n",
    "$p(x_i | \\alpha) = \\dfrac{1} {\\sqrt{2\\pi} \\sigma_i} {\\rm exp} \\left( - \\dfrac{ (x_i - \\mu_i(\\alpha))^2 }{ 2\\sigma_i^2 } \\right)$\n",
    "\n",
    "so we can write the likelihood function as,\n",
    "\n",
    "$\\mathfrak{L}(\\alpha) = e^{-\\chi^2/2} \\times \\prod_{i = 1}^N \\dfrac{ 1 }{ \\sigma_i } \\times (2\\pi)^{-N/2}.$\n",
    "\n",
    "We can now take the natural log of this to get,\n",
    "\n",
    "$-2\\,\\text{ln}\\,\\mathfrak{L} = \\chi^2 + 2\\sum_{i=1}^N\\,\\text{ln}\\,\\sigma_i + N\\,\\text{ln}(2\\pi)$.\n",
    "\n",
    "To maximise $\\mathfrak{L}(\\alpha)$, we then need to minimise $ \\chi^2 + 2\\sum_{i=1}^N\\,\\text{ln}\\,\\sigma_i $. The maximum likelihood $\\alpha_{ML}$ is then the one that satisfies,\n",
    "\n",
    "$\\dfrac{ \\partial }{ \\partial \\alpha } \\left[ -2~\\text{ln}\\,\\mathfrak{L}(\\alpha) \\right]= 0$\n",
    "\n",
    "and the variance of $\\alpha_{ML}$  this given by:\n",
    "\n",
    "$\\text{var}(\\alpha_{ML}) \\approx  \\dfrac{ 2 } { \\dfrac{ \\partial^2 }{ \\partial \\alpha^2 } \\left[ -2~\\text{ln}\\,\\mathfrak{L}(\\alpha) \\right]_{\\alpha=\\alpha_{ML}} }$\n",
    "\n",
    "If we're only interested in the shape of the distribution to estimate statistics such as the mean value of a parameter, we can ignore the additional terms and therefore \n",
    "\n",
    "$-2\\,\\text{ln}\\,\\mathfrak{L} = \\chi^2$ ie\n",
    "\n",
    "$\\text{ln}\\,\\mathfrak{L} \\propto -0.5\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to ask a question from a population but you can’t. You use a sample instead. Previously we took samples from the population using Monte Carlo by assuming a distribution of the sample eg Normal or Binomial. What if you are not happy with assuming a distribution?\n",
    "\n",
    "So we just sample from the sample instead….it is a part of the population after all, just a small discrete part.\n",
    "Given a set of N data points, we randomly draw N numbers from this set:\n",
    "\n",
    "$X=x_1,x_2,x_3,x_4,x_5,x_6,x_7$ \n",
    "\n",
    "$X_{\\rm new}=x_3,x_6,x_7,x_4,x_5,x_1,x_1$. \n",
    "\n",
    "This means that some numbers will be duplicated, and some will be missing entirely (so-called sampling with replacement).  Combining all the bootstrap datasets, we can now get a new estimate of the mean from this new, sampled data set or create N datasets to fit N straight lines or other functions to.\n",
    "\n",
    "So the steps are:\n",
    "\n",
    "- Choose a number of bootstrap samples to perform\n",
    "- Choose a sample size\n",
    "- For each bootstrap sample\n",
    "    - Draw a sample with replacement with the chosen size\n",
    "    - Calculate the statistic on the sample\n",
    "    - Calculate the mean of the calculated sample statistics,\n",
    "    \n",
    "et voila!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "A population of data is randomly generated using `pop=np.random.randint(0,500, size=1000)`. Let's say this is the number of people getting flu twice in one year in the University.\n",
    "\n",
    "Generate a subsample of data 300 points by randomly drawing from this population. This is equivalent to mimicking an experiment where someone works out the average number of people getting flu twice in one year by asking 300 people.\n",
    "\n",
    "- Plot the data and estimate the mean of the population and the mean of the subsample.\n",
    "- Use the bootstrap method to estimate the mean parameter of the population (the mean number of people getting flu twice in one year) and plot your bootstrap sample.  What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "pop = np.random.randint(0,500 , size=100)\n",
    "\n",
    "# draw a sample from population\n",
    "sample = np.random.choice(pop, size=30)\n",
    "\n",
    "#plt the histograms\n",
    "plt.hist(pop,label='population')\n",
    "plt.hist(sample,label='subsample')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# estimate means\n",
    "print('The mean of the population is {:.2f}'.format(pop.mean()))\n",
    "print()\n",
    "print('The mean of the subsample is {:.2f}'.format(sample.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have the same random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# simulated sampling distribution - the bootstrap\n",
    "sample_props = []\n",
    "for _ in range(10000):\n",
    "    sample = np.random.choice(pop, size=30)\n",
    "    sample_props.append(sample.mean())\n",
    "    \n",
    "plt.hist(sample_props,bins=20)\n",
    "plt.axvline(np.mean(sample_props),c='red')\n",
    "plt.show()\n",
    "\n",
    "print('The mean of the bootstrapped sample (ie mean of the means) is {:.2f}'.format(np.mean(sample_props)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the recovered population from the bootstrap looks like a Gaussian and not like the original population which was uniform. This is what we expect however from the Central Limit Theorem in Chapter 3:\n",
    "\n",
    "> When independent random variables are added, their sum tends toward a normal distribution for large N even if the original variables themselves are not normally distributed.\n",
    "\n",
    "The mean of the bootstrap sample is much closer to the mean of the original population indicating that the bootstrap technique is really powerful even when we dont know the original distribution. It also shows how far off the mean of the subsample of 100 people we used for our experiment is from the original population - clearly the subsample was not a good reflection of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to tackle the **Chapter 8 quiz** on Learning Central and the [Chapter 8 yourturn notebook](https://github.com/haleygomez/Data-Science-2025/blob/main/Chapter8/Chapter8_yourturn.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
